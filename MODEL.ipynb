{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "L4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/khaeuss808/NLP_Final_Project_FakeNews/blob/main/MODEL.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ezIt85FYhicD",
        "outputId": "51b163d5-96c5-449c-8ec6-e17fdca112c6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import random\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report\n",
        "from transformers import AutoModel, BertTokenizerFast, BertModel, BertForSequenceClassification,AdamW,BertConfig, get_linear_schedule_with_warmup\n",
        "from torch.utils.data import TensorDataset,DataLoader,RandomSampler,SequentialSampler\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "import torch\n",
        "\n",
        "\n",
        "model=BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=6)\n",
        "tokenizer = BertTokenizerFast.from_pretrained(\"bert-base-uncased\")\n",
        "\n",
        "tokenized_statements=[]\n",
        "\n",
        "liar_df_train=pd.read_csv(\"/content/drive/MyDrive/NLP_Final_Project/train.tsv\",delimiter=\"\\t\")\n",
        "liar_df_valid=pd.read_csv(\"/content/drive/MyDrive/NLP_Final_Project/valid.tsv\",delimiter=\"\\t\")\n",
        "liar_df_test=pd.read_csv(\"/content/drive/MyDrive/NLP_Final_Project/test.tsv\",delimiter=\"\\t\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "total_rows_train=liar_df_train.shape[0]\n",
        "total_rows_valid=liar_df_valid.shape[0]\n",
        "total_rows_test=liar_df_test.shape[0]\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "\n",
        "    # Tell PyTorch to use the GPU.\n",
        "    device = torch.device(\"cuda\")\n",
        "\n",
        "    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n",
        "\n",
        "    print('We will use the GPU:', torch.cuda.get_device_name(0))\n",
        "\n",
        "# If not...\n",
        "else:\n",
        "    print('No GPU available, using the CPU instead.')\n",
        "    device = torch.device(\"cpu\")\n",
        "\n",
        "word_counts = []\n",
        "\n",
        "validation_statements = []\n",
        "\n",
        "test_statements= []"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MFQieaxXiB_c",
        "outputId": "7585c517-32d8-4e08-abbf-a0b1ec91fc34"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "There are 1 GPU(s) available.\n",
            "We will use the GPU: NVIDIA L4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(total_rows_train):\n",
        "    text_train=liar_df_train.iloc[i, 2]\n",
        "    tokens=tokenizer(text_train,padding=\"max_length\",truncation=True,max_length=100, return_tensors=\"pt\")\n",
        "    tokenized_statements.append(tokens)\n",
        "\n",
        "for i in range(total_rows_valid):\n",
        "    text_valid=liar_df_valid.iloc[i, 2]\n",
        "    tokens_valid=tokenizer(text_valid,padding=\"max_length\",truncation=True,max_length=100, return_tensors=\"pt\")\n",
        "    validation_statements.append(tokens_valid)\n",
        "\n",
        "for i in range(total_rows_test):\n",
        "    text_test=liar_df_test.iloc[i, 2]\n",
        "    tokens_test=tokenizer(text_test,padding=\"max_length\",truncation=True,max_length=100, return_tensors=\"pt\")\n",
        "    test_statements.append(tokens_test)\n",
        "\n",
        "train_input_ids = torch.cat([t['input_ids'] for t in tokenized_statements], dim=0)\n",
        "train_attention_mask = torch.cat([t['attention_mask'] for t in tokenized_statements], dim=0)\n",
        "label_encoder = LabelEncoder()\n",
        "numeric_labels = label_encoder.fit_transform(liar_df_train.iloc[:, 1])\n",
        "train_labels = torch.tensor(numeric_labels, dtype=torch.long)\n",
        "\n",
        "validation_input_ids = torch.cat([v['input_ids'] for v in validation_statements], dim=0)\n",
        "validation_attention_mask = torch.cat([v['attention_mask'] for v in validation_statements], dim=0)\n",
        "label_encoder = LabelEncoder()\n",
        "numeric_labels_valid = label_encoder.fit_transform(liar_df_valid.iloc[:, 1])\n",
        "valid_labels = torch.tensor(numeric_labels_valid, dtype=torch.long)\n",
        "\n",
        "test_input_ids = torch.cat([t['input_ids'] for t in test_statements], dim=0)\n",
        "test_attention_mask = torch.cat([t['attention_mask'] for t in test_statements], dim=0)\n",
        "label_encoder_test = LabelEncoder()\n",
        "numeric_labels_test = label_encoder_test.fit_transform(liar_df_test.iloc[:, 1])\n",
        "test_labels = torch.tensor(numeric_labels_test, dtype=torch.long)"
      ],
      "metadata": {
        "id": "67OMNCs103xJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size=32\n",
        "\n",
        "#Training Dataset Preparation\n",
        "train_data=TensorDataset(train_input_ids,train_attention_mask,train_labels)\n",
        "train_sampler=RandomSampler(train_data)\n",
        "train_dataloader=DataLoader(train_data,sampler=train_sampler,batch_size=batch_size)\n",
        "\n",
        "#Double check how to do this without cuda\n",
        "\n",
        "model = model.to(device)  # Moves the model to GPU\n",
        "#input_ids = train_input_ids.cuda()\n",
        "#attention_mask = train_attention_mask.cuda()\n",
        "#labels = train_labels.cuda()\n",
        "\n",
        "optimizer = AdamW(model.parameters(),\n",
        "                  lr = 2e-5,\n",
        "                  eps = 1e-8\n",
        "                )\n",
        "\n",
        "epochs=2\n",
        "total_steps=len(train_dataloader)*epochs\n",
        "\n",
        "scheduler = get_linear_schedule_with_warmup(optimizer,\n",
        "                                            num_warmup_steps=10,\n",
        "                                            num_training_steps=total_steps)\n",
        "\n",
        "def flat_accuracy(preds, labels):\n",
        "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
        "    labels_flat = labels.flatten()\n",
        "    return np.sum(pred_flat == labels_flat) / len(labels_flat)\n",
        "\n",
        "seed_no=24\n",
        "\n",
        "random.seed(seed_no)\n",
        "np.random.seed(seed_no)\n",
        "torch.manual_seed(seed_no)\n",
        "torch.cuda.manual_seed_all(seed_no)\n",
        "\n",
        "loss_values=[]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hKQM8t9x05GB",
        "outputId": "1a75f317-3246-4cbe-f343-b15a67aa23a1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(0,epochs):\n",
        "  print(\"===== Epoch {:}/{:} ====\".format(i+1,epochs))\n",
        "  total_loss=0\n",
        "  model.train()\n",
        "  for step,batch in enumerate(train_dataloader):\n",
        "    b_input_ids = batch[0].to(device)\n",
        "    b_input_mask = batch[1].to(device)\n",
        "    b_labels = batch[2].to(device)\n",
        "    model.zero_grad()\n",
        "    outputs=model(b_input_ids,\n",
        "            token_type_ids=None,\n",
        "            attention_mask=b_input_mask,\n",
        "            labels=b_labels)\n",
        "    loss=outputs[0]\n",
        "    total_loss+=loss.item()\n",
        "    loss.backward()\n",
        "    torch.nn.utils.clip_grad_norm_(model.parameters(),1.0)\n",
        "    optimizer.step()\n",
        "    scheduler.step()\n",
        "#Average training loss calculation\n",
        "train_loss_mean=total_loss/len(train_dataloader)\n",
        "loss_values.append(train_loss_mean)\n",
        "print(\"\")\n",
        "print(\"  Average training loss: {0:.2f}\".format(train_loss_mean))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CzR8_Yo01BWP",
        "outputId": "339487b0-ff7c-470d-d2ac-b57941b2937a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "===== Epoch 1/2 ====\n",
            "===== Epoch 2/2 ====\n",
            "\n",
            "  Average training loss: 1.66\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-vwnZl6f1Dnw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Validation Stage"
      ],
      "metadata": {
        "id": "PE5TBNcM1FAU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Validation phase\n",
        "model.eval()\n",
        "loss_eval, acc_eval=0,0\n",
        "nb_eval_steps,nb_eval_examples=0,0\n",
        "#Data evaluation per epoch\n",
        "##Initialize validation inputs\n",
        "validation_data = TensorDataset(validation_input_ids, validation_attention_mask, valid_labels)\n",
        "validation_sampler = SequentialSampler(validation_data)\n",
        "validation_dataloader = DataLoader(validation_data, sampler=validation_sampler, batch_size=batch_size)\n",
        "\n",
        "for batch in validation_dataloader:\n",
        "   batch = tuple(t.to(device) for t in batch)\n",
        "   #Unpack inputs from dataloader\n",
        "   v_input_ids,v_input_mask,v_labels=batch\n",
        "\n",
        "   #Do not store gradients here\n",
        "   with torch.no_grad():\n",
        "      outputs=model(v_input_ids,token_type_ids=None,attention_mask=v_input_mask)\n",
        "   logits=outputs[0]\n",
        "   logits = logits.detach().cpu().numpy()\n",
        "   label_ids = v_labels.to('cpu').numpy()\n",
        "   eval_accuracy_batch=flat_accuracy(logits,label_ids)\n",
        "   acc_eval+=eval_accuracy_batch\n",
        "   nb_eval_steps+=1\n",
        "\n",
        "\n",
        "#Report accuracy of validation run\n",
        "print(\"Accuracy:{0:.2f}\".format(acc_eval/nb_eval_steps))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MjiuaFem1KTK",
        "outputId": "64b1cf07-0c2e-4477-f27d-e33984f5af0a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy:0.26\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Testing Stage"
      ],
      "metadata": {
        "id": "xp6uqt5M1OS_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"#Test phase\n",
        "model.eval()\n",
        "loss_test, acc_test=0,0\n",
        "nb_test_steps,nb_test_examples=0,0\n",
        "\n",
        "# Tracking variables\n",
        "predictions , true_labels = [], []\n",
        "\n",
        "#Data evaluation per epoch\n",
        "##Initialize validation inputs\n",
        "test_data = TensorDataset(test_input_ids, test_attention_mask, test_labels)\n",
        "test_sampler = SequentialSampler(test_data)\n",
        "test_dataloader = DataLoader(test_data, sampler=test_sampler, batch_size=batch_size)\n",
        "\n",
        "for batch in test_dataloader:\n",
        "   batch = tuple(t.to(device) for t in batch)\n",
        "   #Unpack inputs from dataloader\n",
        "   t_input_ids,t_input_mask,t_labels=batch\n",
        "\n",
        "   #Do not store gradients here\n",
        "   with torch.no_grad():\n",
        "      outputs=model(t_input_ids,token_type_ids=None,attention_mask=t_input_mask)\n",
        "   logits=outputs[0]\n",
        "   logits = logits.detach().cpu().numpy()\n",
        "   label_ids = t_labels.to('cpu').numpy()\n",
        "   test_accuracy_batch=flat_accuracy(logits,label_ids)\n",
        "   acc_test+=test_accuracy_batch\n",
        "   nb_test_steps+=1\n",
        "\n",
        "\n",
        "#Report accuracy of validation run\n",
        "print(\"Accuracy:{0:.2f}\".format(acc_test/nb_test_steps))\n",
        "\"\"\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 209
        },
        "id": "CAMQEBXt1T7H",
        "outputId": "bc5d17a3-90b3-4290-f8f2-6ab90f5dfee5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'#Test phase\\nmodel.eval()\\nloss_test, acc_test=0,0\\nnb_test_steps,nb_test_examples=0,0\\n\\n# Tracking variables\\npredictions , true_labels = [], []\\n\\n#Data evaluation per epoch\\n##Initialize validation inputs\\ntest_data = TensorDataset(test_input_ids, test_attention_mask, test_labels)\\ntest_sampler = SequentialSampler(test_data)\\ntest_dataloader = DataLoader(test_data, sampler=test_sampler, batch_size=batch_size)\\n\\nfor batch in test_dataloader:\\n   batch = tuple(t.to(device) for t in batch)\\n   #Unpack inputs from dataloader\\n   t_input_ids,t_input_mask,t_labels=batch\\n\\n   #Do not store gradients here\\n   with torch.no_grad():\\n      outputs=model(t_input_ids,token_type_ids=None,attention_mask=t_input_mask)\\n   logits=outputs[0]\\n   logits = logits.detach().cpu().numpy()\\n   label_ids = t_labels.to(\\'cpu\\').numpy()\\n   test_accuracy_batch=flat_accuracy(logits,label_ids)\\n   acc_test+=test_accuracy_batch\\n   nb_test_steps+=1\\n\\n\\n#Report accuracy of validation run\\nprint(\"Accuracy:{0:.2f}\".format(acc_test/nb_test_steps))\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Prepare the single statement\n",
        "def predict_single_statement(model, tokenizer, statement, device):\n",
        "    model.eval()  # Set the model to evaluation mode\n",
        "\n",
        "    # Tokenize and encode the statement\n",
        "    encoded_dict = tokenizer.encode_plus(\n",
        "        statement,\n",
        "        add_special_tokens=True,\n",
        "        max_length=100,  # Adjust as per your model's input requirements\n",
        "        pad_to_max_length=True,\n",
        "        return_attention_mask=True,\n",
        "        return_tensors='pt',  # Return PyTorch tensors\n",
        "    )\n",
        "\n",
        "    # Extract input IDs and attention masks\n",
        "    input_ids_test = encoded_dict['input_ids'].to(device)\n",
        "    attention_mask_test = encoded_dict['attention_mask'].to(device)\n",
        "\n",
        "    # Perform the prediction\n",
        "    with torch.no_grad():\n",
        "        outputs = model(input_ids_test, token_type_ids=None, attention_mask=attention_mask_test)\n",
        "\n",
        "    # Extract logits\n",
        "    logits = outputs[0]\n",
        "    logits = logits.detach().cpu().numpy()\n",
        "\n",
        "    # Get the predicted class\n",
        "    predicted_class = logits.argmax(axis=1)[0]\n",
        "    predicted_numeric_class=predicted_class\n",
        "\n",
        "    predicted_string_label = label_encoder_test.inverse_transform([predicted_numeric_class])[0]\n",
        "\n",
        "    return predicted_string_label, logits\n",
        "\n",
        "# Example usage\n",
        "statement = \"John McCain is the current president\"\n",
        "predicted_string_label, logits = predict_single_statement(model, tokenizer, statement, device)\n",
        "print(f\"Predicted String Class: {predicted_string_label}\")\n",
        "print(f\"Logits: {logits}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VBjOAmkb6wLs",
        "outputId": "f7ba7e72-0da1-4494-c21a-3be853088bf4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicted String Class: mostly-true\n",
            "Logits: [[-0.05726965  0.10686778  0.46787152  0.7926157  -0.9533817   0.71926993]]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2834: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    }
  ]
}